{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7029255,"sourceType":"datasetVersion","datasetId":4043000},{"sourceId":7029647,"sourceType":"datasetVersion","datasetId":4043266},{"sourceId":7029648,"sourceType":"datasetVersion","datasetId":4043267},{"sourceId":7033720,"sourceType":"datasetVersion","datasetId":4046081},{"sourceId":7034607,"sourceType":"datasetVersion","datasetId":4046743},{"sourceId":7035258,"sourceType":"datasetVersion","datasetId":4047234}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"--------------------------------------------------------------------","metadata":{}},{"cell_type":"markdown","source":"Fully-Connected Neural Network","metadata":{}},{"cell_type":"code","source":"from sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn import preprocessing\nfrom keras.optimizers import Adam\nimport pandas as pd\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import LabelEncoder\n\nimport numpy as np\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.optimizers import Adam\nfrom keras.losses import SparseCategoricalCrossentropy\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy\nfrom sklearn.model_selection import train_test_split\ndef trainModel(model, epochs, optimizer, X_train, y_train):\n    batch_size = 128\n    model.compile(optimizer=optimizer, loss=SparseCategoricalCrossentropy(), metrics=['accuracy'])\n    return model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size)\n\ndef plotTrainingHistory(history):\n    print(\"Training accuracy:\", max(history.history['accuracy']))\n    pd.DataFrame(history.history).plot(figsize=(12, 6))\n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-11-25T12:17:11.546700Z","iopub.execute_input":"2023-11-25T12:17:11.547385Z","iopub.status.idle":"2023-11-25T12:17:11.557197Z","shell.execute_reply.started":"2023-11-25T12:17:11.547354Z","shell.execute_reply":"2023-11-25T12:17:11.556186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"30sec Dataset FNN ","metadata":{}},{"cell_type":"code","source":"# train_data = pd.read_csv('/kaggle/input/3trainsec/features_3_sec_train.csv')\n# train_data = train_data.iloc[:, 2:]  # Exclude the first column if it's just an index\n# y_train = train_data['label']  # genre variable\n# X_train = train_data.loc[:, train_data.columns != 'label']  # Select all columns except the labels\n\n# # Normalize X_train\n# cols = X_train.columns\n# min_max_scaler = preprocessing.MinMaxScaler()\n# np_scaled = min_max_scaler.fit_transform(X_train)\n# X_train = pd.DataFrame(np_scaled, columns=cols)\n\n# # Load the test data from \"features_3_sec_test.csv\"\n# test_data = pd.read_csv('/kaggle/input/3testsec/features_3_sec_test.csv')\n# test_data = test_data.iloc[:, 2:]  # Exclude the first column if it's just an index\n# y_test = test_data['label']  # genre variable\n# X_test = test_data.loc[:, test_data.columns != 'label']  # Select all columns except the labels\n\n# # Normalize X_test\n# np_scaled = min_max_scaler.transform(X_test)\n# X_test = pd.DataFrame(np_scaled, columns=cols)\n\n\n# le = LabelEncoder()\n# y_train = le.fit_transform(y_train)\n# y_test = le.transform(y_test)\n\n\ndata = pd.read_csv('/kaggle/input/30secdata/features_30_sec (1).csv')\ndata = data.iloc[:, 2:]\ny = data['label']  \nX = data.loc[:, data.columns != 'label']\n\n# Normalize X\ncols = X.columns\nmin_max_scaler = preprocessing.MinMaxScaler()\nnp_scaled = min_max_scaler.fit_transform(X)\nX = pd.DataFrame(np_scaled, columns=cols)\n\n# Encode labels using LabelEncoder\nle = LabelEncoder()\ny = le.fit_transform(y)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n\nmodel = Sequential([\n    Dense(512, activation='relu', input_shape=(X_train.shape[1],)),\n    Dropout(0.2),\n\n    Dense(256, activation='relu'),\n    Dropout(0.2),\n\n    Dense(64, activation='relu'),\n    Dropout(0.2),\n\n    Dense(10, activation='softmax')\n])\nprint(model.summary())\nadam_optimizer = Adam()\n\n# Compile the model\nmodel.compile(optimizer=adam_optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Training the model on the training set\nmodel_history = trainModel(model=model, epochs=300, optimizer=adam_optimizer, X_train=X_train, y_train=y_train)\n\n# Evaluating the model on the test set\ntest_loss, test_accuracy = model.evaluate(X_test, y_test)\nprint(\"Test Loss:\", test_loss)\nprint(\"Test Accuracy:\", test_accuracy)\n\n# Predictions on the test set\ny_pred = model.predict(X_test)\n\n# Convert one-hot encoded labels back to categorical labels\ny_test_labels = np.argmax(y_test, axis=1)\ny_pred_labels = np.argmax(y_pred, axis=1)\n\n# Compute precision, recall, and F1 score\nprecision = precision_score(y_test_labels, y_pred_labels, average='weighted')\nrecall = recall_score(y_test_labels, y_pred_labels, average='weighted')\nf1 = f1_score(y_test_labels, y_pred_labels, average='weighted')\n\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1 Score:\", f1)","metadata":{"execution":{"iopub.status.busy":"2023-11-25T12:17:35.491390Z","iopub.execute_input":"2023-11-25T12:17:35.492255Z","iopub.status.idle":"2023-11-25T12:17:50.171071Z","shell.execute_reply.started":"2023-11-25T12:17:35.492221Z","shell.execute_reply":"2023-11-25T12:17:50.169812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_embeddings = model.predict(X_test)\n\n# Calculate cosine similarity\ncosine_sim = cosine_similarity(test_embeddings)\n\n# Create a heatmap\nplt.figure(figsize=(10, 8))\nsns.heatmap(cosine_sim, annot=True, cmap='viridis', fmt=\".2f\", xticklabels=False, yticklabels=False)\nplt.title('Cosine Similarity Heatmap')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-24T05:47:39.350798Z","iopub.execute_input":"2023-11-24T05:47:39.351203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.layers import Dense, Dropout\nfrom keras.models import Sequential, Model\nfrom sklearn.metrics.pairwise import cosine_similarity\nlayer_index = -1\n\n# ... (your existing model definition)\n\n# Create an intermediate model for getting layer outputs\nintermediate_layer_model = Model(inputs=model.input, outputs=model.layers[layer_index].output)\n\n# Get embeddings for training and test data\nX_train_embeddings = intermediate_layer_model.predict(X_train)\nX_test_embeddings = intermediate_layer_model.predict(X_test)\n\n# Compute cosine similarity matrix\nsimilarity_matrix = cosine_similarity(X_test_embeddings, X_train_embeddings)\n\n# Get the indices of most similar samples and their corresponding similarity scores\nmost_similar_indices = similarity_matrix.argmax(axis=1)\nsimilarity_scores = similarity_matrix.max(axis=1)\n\n# Display the results\nfor i, (test_index, similarity_score) in enumerate(zip(range(len(X_test)), similarity_scores)):\n    train_index = most_similar_indices[i]\n    print(f\"Test sample {test_index} is most similar to train sample {train_index} with a similarity score of {similarity_score}\")","metadata":{"execution":{"iopub.status.busy":"2023-11-24T05:39:01.843739Z","iopub.execute_input":"2023-11-24T05:39:01.844474Z","iopub.status.idle":"2023-11-24T05:39:02.311455Z","shell.execute_reply.started":"2023-11-24T05:39:01.844437Z","shell.execute_reply":"2023-11-24T05:39:02.309889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# ... (your existing code to compute the cosine similarity matrix)\n\n# Plot the cosine similarity matrix as a heatmap\nplt.figure(figsize=(10, 8))\nsns.heatmap(similarity_matrix, cmap='viridis', annot=True, fmt=\".2f\", xticklabels=False, yticklabels=False)\nplt.title('Cosine Similarity Matrix')\nplt.xlabel('Training Set Samples')\nplt.ylabel('Test Set Samples')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-24T05:43:02.083480Z","iopub.execute_input":"2023-11-24T05:43:02.083958Z","iopub.status.idle":"2023-11-24T05:45:37.710888Z","shell.execute_reply.started":"2023-11-24T05:43:02.083924Z","shell.execute_reply":"2023-11-24T05:45:37.708807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import confusion_matrix, classification_report\n\n# Assuming X_test and y_test are your test data and labels\n\n# Predict probabilities for each class\ny_pred_probabilities = model.predict(X_test)\n\n# Convert probabilities to class predictions\ny_pred_classes = np.argmax(y_pred_probabilities, axis=1)\n\n# If your labels are one-hot encoded, convert them to classes\nif len(y_test.shape) > 1:\n    y_true_classes = np.argmax(y_test, axis=1)\nelse:\n    y_true_classes = y_test\n\n# Calculate confusion matrix\nconf_matrix = confusion_matrix(y_true_classes, y_pred_classes)\n\n# Calculate precision, recall, and F1 score for each class\nprecision = np.diag(conf_matrix) / np.sum(conf_matrix, axis=0)\nrecall = np.diag(conf_matrix) / np.sum(conf_matrix, axis=1)\nf1_score = 2 * (precision * recall) / (precision + recall)\n\n# Print results for each class\nfor i in range(len(precision)):\n    print(f\"Class {i}:\")\n    print(f\"  Precision: {precision[i]}\")\n    print(f\"  Recall: {recall[i]}\")\n    print(f\"  F1 Score: {f1_score[i]}\")\n\n# You can also use sklearn's classification report for a summary\nclassification_report_str = classification_report(y_true_classes, y_pred_classes)\nprint(\"Classification Report:\\n\", classification_report_str)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-23T21:22:34.725690Z","iopub.execute_input":"2023-11-23T21:22:34.726456Z","iopub.status.idle":"2023-11-23T21:22:34.832888Z","shell.execute_reply.started":"2023-11-23T21:22:34.726424Z","shell.execute_reply":"2023-11-23T21:22:34.831985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define and compile the model\ninput_shape = X_train.shape[1:]  # Adjust according to your input data shape\nnum_classes = len(np.unique(y))  # Assuming y contains class labels\ncombined_model = build_combined_model(input_shape)\ncombined_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Train the model\nbatch_size = 32\nepochs = 10\ncombined_model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n\n# Evaluate the model on the test set\neval_metrics = combined_model.evaluate(X_test, y_test)\nprint(\"Test Loss:\", eval_metrics[0])\nprint(\"Test Accuracy:\", eval_metrics[1])","metadata":{"execution":{"iopub.status.busy":"2023-11-23T19:34:54.999612Z","iopub.execute_input":"2023-11-23T19:34:55.000031Z","iopub.status.idle":"2023-11-23T19:34:55.199331Z","shell.execute_reply.started":"2023-11-23T19:34:54.999995Z","shell.execute_reply":"2023-11-23T19:34:55.197509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}